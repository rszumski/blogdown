<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on szumskiR</title>
    <link>/post/</link>
    <description>Recent content in Posts on szumskiR</description>
    <generator>Hugo -- gohugo.io</generator>
    <managingEditor>rszumski@gmail.com (Russell Szumski)</managingEditor>
    <webMaster>rszumski@gmail.com (Russell Szumski)</webMaster>
    <lastBuildDate>Sun, 04 Feb 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Web Scraping - The Numbers</title>
      <link>/post/web-scraping-the-numbers/</link>
      <pubDate>Sun, 04 Feb 2018 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/web-scraping-the-numbers/</guid>
      <description>A friend of mine was looking for data dealing with movie budget and performance, so I thought it would be a good opportunity to practice web scrapping using the rvest package.
I be getting the data from The Numbers. The goal is to obtain the release date, movie title, production budget, domestic gross, and worldwide gross from there Budget and Financial page. Then join that with the distributor, genre, and movie rating from the MPAA Ratings page.</description>
    </item>
    
    <item>
      <title>US Wildfires by County</title>
      <link>/post/us-wildfires-by-county/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/us-wildfires-by-county/</guid>
      <description>I watched Only the Brave, a 2017 American biographical action drama film that tells the true story of the Granite Mountain Hotshots, which lead me to the Kaggle 1.88 Million US Wildfires data set, then on to one of the Kernels titled Wildfire Exploratory Analysis which had some interesting analysis of wildfires in the US and a cool choropleth map of total US wildfires by County. That lead me to the highmaps section of highcharter, and finally I found my way to a ‘adding motion to choropleths’ blog post on jkunst that replicated a visualization made by New York Times showing How the Epidemic of Drug Overdose Deaths Rippled Across America.</description>
    </item>
    
    <item>
      <title>A Hertzsprung–Russell Diagram</title>
      <link>/post/hr-diagram/</link>
      <pubDate>Thu, 25 Jan 2018 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/hr-diagram/</guid>
      <description>Fact, Hertzsprung–Russell diagrams (HR diagrams) are cool. They are one of the most important tools in the study of the way that stars change with time. Developed independently in the early 1900s by Ejnar Hertzsprung and Henry Norris Russell, it plots each star on a graph measuring the star’s brightness against its temperature (color). Depending on its initial mass, every star goes through specific evolutionary stages dictated by its internal structure and how it produces energy.</description>
    </item>
    
    <item>
      <title> I Want to Believe</title>
      <link>/post/ufo-sightings-around-the-world/</link>
      <pubDate>Wed, 17 Jan 2018 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/ufo-sightings-around-the-world/</guid>
      <description>I’ve been watching the 11th season of The X-Files and it got me thinking about UFOs. In my search for data on UFO sightings I came across a dataset on Kaggle that has 80,000+ documented close encounters from the past 70 years. I also came across a R package called dygraphs, for charting time-series data, and the three.js package, that has globejs: a somewhat silly widget that plots data and images on a 3-d globe.</description>
    </item>
    
    <item>
      <title>Infinite Monkey vs. R</title>
      <link>/post/text-generator/</link>
      <pubDate>Tue, 16 Jan 2018 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/text-generator/</guid>
      <description>The infinite monkey theorem states that a monkey hitting keys at random on a typewriter for an infinite amount of time will almost surely type something that makes sense. I don’t have a monkey or an infinite amount of time, but can I make somewhat understandable sentences by sticking commonly related words together? Maybe.
Can we do it by sticking commonly used words together in Donald Trumps tweets?
A lot of his tweets already sound nonsensical to begin with, so let,s give it a try.</description>
    </item>
    
    <item>
      <title>The Flow of People</title>
      <link>/post/migration/</link>
      <pubDate>Mon, 15 Jan 2018 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/migration/</guid>
      <description>There has been a lot of talk about immigration lately. President Trump balked at an immigration deal that would include protections for people from Haiti and nations in Africa, demanding to know at a White House meeting why he should accept immigrants from “shithole countries” rather than from places like Norway, according to people with direct knowledge of the conversation.
What percent of people that come to the US come from “shithole countries”?</description>
    </item>
    
    <item>
      <title>Instant Ramen</title>
      <link>/post/ramen-bigrams/</link>
      <pubDate>Wed, 10 Jan 2018 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/ramen-bigrams/</guid>
      <description>A dataset of 2682 Instant Noodle Ratings was poseted on the datasets subreddit. So I created bi-grams to see how often word X is followed by word Y with names of instant ramen noodles to find any flavors and key attributes/phrases used in the names that are grouped together and appear most often. Why? Why not, who hasn’t enjoyed some instant ramen before.
Used tidytext for the calculations and networkD3 for the visualization.</description>
    </item>
    
    <item>
      <title>Sad Songs</title>
      <link>/post/gregory-alan-isakov/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/gregory-alan-isakov/</guid>
      <description>Gregory Alan Isakov has been one of my favorite singer-songwriters for a while, and I’ve gotten used to people suggesting that I play something ‘less depressing’. Most singer-songwriter’s music is indeed sad, but his music can’t be that sad. Can it?
Using the spotifyr package we can pull track audio features from the ‘Spotify’ Web API, including valence and energy of a track.
Valence: a measure from 0.</description>
    </item>
    
    <item>
      <title>Weather Is Not Climate</title>
      <link>/post/annual-temp-dev/</link>
      <pubDate>Wed, 20 Dec 2017 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/annual-temp-dev/</guid>
      <description>I live in Chicago and it’s cold old, but weather is not climate. The difference between weather and climate is a measure of time. Weather is what conditions of the atmosphere are over a short period of time, and climate is how the atmosphere “behaves” over relatively long periods of time.
Let’s take the data and plot the deviations from the 1951 - 1980 average going back to 1880 and see what we get.</description>
    </item>
    
    <item>
      <title>Inaugural Addresses are more ... unintelligible</title>
      <link>/post/inaugural-address-smog/</link>
      <pubDate>Sun, 03 Dec 2017 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/inaugural-address-smog/</guid>
      <description>I come across an interesting infographic from the Guardian interactive team. It used the Flesch-Kincaid readability test to track the reading level of every State of the Union. I thought I’d give it a go and try to create something like it.
Instead of using the Flesch-Kincaid readability test I went with the Simple Measure of Gobbledygook (SMOG) grade, which is a measure of readability that estimates the years of education needed to understand a piece of writing, and I just looked at the Inaugural Addresses.</description>
    </item>
    
    <item>
      <title>Cluster Analysis of Selected Socioeconomic Indicators in Chicago</title>
      <link>/post/chicago-census-cluster/</link>
      <pubDate>Thu, 18 May 2017 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/chicago-census-cluster/</guid>
      <description>K-mean and PAM (Partitioning Around Medoids) cluster analysis of Census Data with selected socioeconomic indicators in Chicago from 2008 – 2012.
The dataset This dataset contains a selection of six socioeconomic indicators of public health significance and a “hardship index,” by Chicago community area, for the years 2008 – 2012. The indicators are the percent of occupied housing units with more than one person per room (i.e., crowded housing); the percent of households living below the federal poverty level; the percent of persons in the labor force over the age of 16 years that are unemployed; the percent of persons over the age of 25 years without a high school diploma; the percent of the population under 18 or over 64 years of age (i.</description>
    </item>
    
    <item>
      <title>Donald Trump&#39;s sad iPhone Tweets</title>
      <link>/post/tweet-analysis/</link>
      <pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate>
      <author>rszumski@gmail.com (Russell Szumski)</author>
      <guid>/post/tweet-analysis/</guid>
      <description>White House director of social media Dan Scavino Jr. tweeted that Trump had switched his personal device to an iPhone. In the past Trump tweeted from his Andriod device and his staff from an iPhone. It was shown that the Android tweets where angrier and more negative.
Since the switch to an iPhone have the overall iPhone tweets become angrier and more negative?
The data It was reported that Trump got his new iPhone on March 25th, so first we’ll get the past 600 tweets from the realDonaldTrump using the twitteR package for R.</description>
    </item>
    
  </channel>
</rss>